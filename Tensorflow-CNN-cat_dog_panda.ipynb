{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import img data and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分別抓取指定路徑下所有圖片位置，再貼上標籤(cat:1,dog:2,panda:3)\n",
    "df1 = pd.DataFrame(glob.glob('C:/Users/User/OneDrive/dataset/cats/*'))\n",
    "df1['label']=0\n",
    "df2 = pd.DataFrame(glob.glob('C:/Users/User/OneDrive/dataset/dogs/*'))\n",
    "df2['label']=1\n",
    "df3 = pd.DataFrame(glob.glob('C:/Users/User/OneDrive/dataset/panda/*'))\n",
    "df3['label']=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#合併三個資料夾中所有路徑\n",
    "df = pd.concat([df1,df2,df3],axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#將答案提出來\n",
    "y=df['label'].values.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#將指定路徑所有圖片用opencv讀取並resize成32x32，再彙整再一起\n",
    "x = []\n",
    "for i in df[0]:\n",
    "    img = cv2.imread(i)\n",
    "    img = cv2.resize(img,(64,64))\n",
    "    x.append(img)\n",
    "#將圖片資料格式指定成float可以提升訓練model效率\n",
    "x = np.array(x,dtype='float')\n",
    "#將data normalize\n",
    "x = x/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分割train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D,MaxPool2D,Dropout,Flatten,Dense,BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_jmda = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_25 (Conv2D)           (None, 64, 64, 64)        1792      \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 16384)             65536     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 512)               8389120   \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 9,635,395\n",
      "Trainable params: 9,601,475\n",
      "Non-trainable params: 33,920\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_jmda.add(Conv2D(filters=64,kernel_size=3,padding='same',activation='relu',input_shape=(64,64,3)))\n",
    "model_jmda.add(Conv2D(filters=64,kernel_size=3,padding='same',activation='relu'))\n",
    "model_jmda.add(MaxPool2D(pool_size=(2, 2),padding='same'))\n",
    "model_jmda.add(Conv2D(filters=128,kernel_size=3,padding='same',activation='relu'))\n",
    "model_jmda.add(Conv2D(filters=128,kernel_size=3,padding='same',activation='relu'))\n",
    "model_jmda.add(MaxPool2D(pool_size=(2, 2),padding='same'))\n",
    "model_jmda.add(Conv2D(filters=256,kernel_size=3,padding='same',activation='relu'))\n",
    "model_jmda.add(Conv2D(filters=256,kernel_size=3,padding='same',activation='relu'))\n",
    "model_jmda.add(MaxPool2D(pool_size=(2, 2),padding='same'))\n",
    "model_jmda.add(Flatten())\n",
    "model_jmda.add(BatchNormalization())\n",
    "model_jmda.add(Dense(512,activation='relu'))\n",
    "model_jmda.add(BatchNormalization())\n",
    "model_jmda.add(Dropout(0.3))\n",
    "model_jmda.add(Dense(64,activation='relu'))\n",
    "model_jmda.add(BatchNormalization())\n",
    "model_jmda.add(Dense(3,activation='softmax'))\n",
    "model_jmda.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_jmda.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2250 samples, validate on 750 samples\n",
      "Epoch 1/20\n",
      "2250/2250 [==============================] - 89s 40ms/sample - loss: 0.9899 - sparse_categorical_accuracy: 0.5684 - val_loss: 2.1365 - val_sparse_categorical_accuracy: 0.4680\n",
      "Epoch 2/20\n",
      "2250/2250 [==============================] - 94s 42ms/sample - loss: 0.6561 - sparse_categorical_accuracy: 0.6787 - val_loss: 10.6479 - val_sparse_categorical_accuracy: 0.3107\n",
      "Epoch 3/20\n",
      "2250/2250 [==============================] - 93s 41ms/sample - loss: 0.6108 - sparse_categorical_accuracy: 0.7067 - val_loss: 4.1587 - val_sparse_categorical_accuracy: 0.3800\n",
      "Epoch 4/20\n",
      "2250/2250 [==============================] - 87s 39ms/sample - loss: 0.5643 - sparse_categorical_accuracy: 0.7440 - val_loss: 0.8437 - val_sparse_categorical_accuracy: 0.6013\n",
      "Epoch 5/20\n",
      "2250/2250 [==============================] - 82s 36ms/sample - loss: 0.4988 - sparse_categorical_accuracy: 0.7804 - val_loss: 0.7723 - val_sparse_categorical_accuracy: 0.6320\n",
      "Epoch 6/20\n",
      "2250/2250 [==============================] - 87s 39ms/sample - loss: 0.4376 - sparse_categorical_accuracy: 0.8111 - val_loss: 0.8129 - val_sparse_categorical_accuracy: 0.5640\n",
      "Epoch 7/20\n",
      "2250/2250 [==============================] - 81s 36ms/sample - loss: 0.4041 - sparse_categorical_accuracy: 0.8249 - val_loss: 0.8244 - val_sparse_categorical_accuracy: 0.6120\n",
      "Epoch 8/20\n",
      "2250/2250 [==============================] - 82s 36ms/sample - loss: 0.3319 - sparse_categorical_accuracy: 0.8680 - val_loss: 0.8149 - val_sparse_categorical_accuracy: 0.6053\n",
      "Epoch 9/20\n",
      "2250/2250 [==============================] - 81s 36ms/sample - loss: 0.2540 - sparse_categorical_accuracy: 0.9076 - val_loss: 0.7509 - val_sparse_categorical_accuracy: 0.6600\n",
      "Epoch 10/20\n",
      "2250/2250 [==============================] - 81s 36ms/sample - loss: 0.2025 - sparse_categorical_accuracy: 0.9342 - val_loss: 0.8329 - val_sparse_categorical_accuracy: 0.5920\n",
      "Epoch 11/20\n",
      "2250/2250 [==============================] - 81s 36ms/sample - loss: 0.1569 - sparse_categorical_accuracy: 0.9516 - val_loss: 1.1020 - val_sparse_categorical_accuracy: 0.5640\n",
      "Epoch 12/20\n",
      "2250/2250 [==============================] - 82s 36ms/sample - loss: 0.1275 - sparse_categorical_accuracy: 0.9618 - val_loss: 0.9055 - val_sparse_categorical_accuracy: 0.6067\n",
      "Epoch 13/20\n",
      "2250/2250 [==============================] - 82s 36ms/sample - loss: 0.0928 - sparse_categorical_accuracy: 0.9760 - val_loss: 0.7910 - val_sparse_categorical_accuracy: 0.6587\n",
      "Epoch 14/20\n",
      "2250/2250 [==============================] - 81s 36ms/sample - loss: 0.0615 - sparse_categorical_accuracy: 0.9880 - val_loss: 0.7890 - val_sparse_categorical_accuracy: 0.6813\n",
      "Epoch 15/20\n",
      "2250/2250 [==============================] - 82s 36ms/sample - loss: 0.0445 - sparse_categorical_accuracy: 0.9907 - val_loss: 0.9219 - val_sparse_categorical_accuracy: 0.6653\n",
      "Epoch 16/20\n",
      "2250/2250 [==============================] - 82s 36ms/sample - loss: 0.0451 - sparse_categorical_accuracy: 0.9889 - val_loss: 1.0220 - val_sparse_categorical_accuracy: 0.6467\n",
      "Epoch 17/20\n",
      "2250/2250 [==============================] - 80s 36ms/sample - loss: 0.0479 - sparse_categorical_accuracy: 0.9867 - val_loss: 1.0204 - val_sparse_categorical_accuracy: 0.6747\n",
      "Epoch 18/20\n",
      "2250/2250 [==============================] - 86s 38ms/sample - loss: 0.0346 - sparse_categorical_accuracy: 0.9933 - val_loss: 1.5427 - val_sparse_categorical_accuracy: 0.5773\n",
      "Epoch 19/20\n",
      "2250/2250 [==============================] - 91s 41ms/sample - loss: 0.0318 - sparse_categorical_accuracy: 0.9942 - val_loss: 1.2967 - val_sparse_categorical_accuracy: 0.6133\n",
      "Epoch 20/20\n",
      "2250/2250 [==============================] - 92s 41ms/sample - loss: 0.0319 - sparse_categorical_accuracy: 0.9942 - val_loss: 1.3268 - val_sparse_categorical_accuracy: 0.6387\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14ba89442e8>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_jmda.fit(x_train,y_train,batch_size=100,epochs=20,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict = pd.DataFrame(glob.glob('C:/Users/User/OneDrive/dataset/test/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_predict = []\n",
    "for i in df_predict[0]:\n",
    "    x_p = cv2.imread(i)\n",
    "    x_p = cv2.resize(x_p,(64,64))\n",
    "    x_predict.append(x_p)\n",
    "x_predict = np.array(x_predict,dtype='float')\n",
    "x_predict = x_predict/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      dog\n",
       "1      dog\n",
       "2    panda\n",
       "dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_animal = {0:'cat',1:'dog',2:'panda'}\n",
    "matrix = np.round(model_jmda.predict(x_predict),2)\n",
    "answer = pd.Series(model_jmda.predict_classes(x_predict))\n",
    "answer.map(dict_animal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07, 0.93, 0.  ],\n",
       "       [0.1 , 0.87, 0.03],\n",
       "       [0.  , 0.  , 1.  ]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
